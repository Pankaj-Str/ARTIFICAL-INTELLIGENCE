{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73786f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\programming\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image from URL: 403 Client Error: Forbidden. Please comply with the User-Agent policy: https://meta.wikimedia.org/wiki/User-Agent_policy for url: https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Juvenile_Ragdoll.jpg/640px-Juvenile_Ragdoll.jpg\n",
      "Image loading failed. Exiting.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     56\u001b[39m     exit()\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# =============== Step 4: Preprocess and Predict ===============\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m img_t = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m batch_t = torch.unsqueeze(img_t, \u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# add batch dimension\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\transforms\\functional.py:465\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) != \u001b[32m1\u001b[39m:\n\u001b[32m    460\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    461\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m _, image_height, image_width = \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    467\u001b[39m     size = [size]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\transforms\\functional.py:80\u001b[39m, in \u001b[36mget_dimensions\u001b[39m\u001b[34m(img)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t.get_dimensions(img)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Programming\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[39m, in \u001b[36mget_dimensions\u001b[39m\u001b[34m(img)\u001b[39m\n\u001b[32m     29\u001b[39m     width, height = img.size\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Unexpected type <class 'NoneType'>"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# =============== Step 1: Load Pre-trained InceptionV3 Model ===============\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# =============== Step 2: Define Preprocessing ===============\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet standards\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# =============== Step 3: Load Image ===============\n",
    "\n",
    "# Option 1: Load from URL\n",
    "def load_image_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(\"Error loading image from URL:\", e)\n",
    "        return None\n",
    "\n",
    "# Option 2: Load local image\n",
    "def load_image_from_file(file_path):\n",
    "    try:\n",
    "        img = Image.open(file_path).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(\"Error loading local image:\", e)\n",
    "        return None\n",
    "\n",
    "# Choose one of the options below:\n",
    "# 1. From URL\n",
    "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Juvenile_Ragdoll.jpg/640px-Juvenile_Ragdoll.jpg\"\n",
    "img = load_image_from_url(img_url)\n",
    "\n",
    "# 2. From local file (uncomment to use local file)\n",
    "# img_path = \"your_image.jpg\"\n",
    "# img = load_image_from_file(img_path)\n",
    "\n",
    "if img is None:\n",
    "    print(\"Image loading failed. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# =============== Step 4: Preprocess and Predict ===============\n",
    "img_t = preprocess(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0)  # add batch dimension\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(batch_t)\n",
    "\n",
    "# =============== Step 5: Decode Predictions ===============\n",
    "# Download labels from ImageNet if not available\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "labels_path = \"imagenet_classes.txt\"\n",
    "\n",
    "if not os.path.exists(labels_path):\n",
    "    with open(labels_path, \"w\") as f:\n",
    "        f.write(requests.get(LABELS_URL).text)\n",
    "\n",
    "# Read class names\n",
    "with open(labels_path) as f:\n",
    "    categories = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Softmax and Top-5 predictions\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "\n",
    "print(\"\\nTop-5 Predictions:\")\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(f\"{categories[top5_catid[i]]}: {top5_prob[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfb67bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download image. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Use a working image URL\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Juvenile_Ragdoll.jpg/640px-Juvenile_Ragdoll.jpg\"\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if image downloaded successfully\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        img.show()  # optional: show image to verify\n",
    "    except Exception as e:\n",
    "        print(\"Failed to open image:\", e)\n",
    "else:\n",
    "    print(\"Failed to download image. Status code:\", response.status_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
